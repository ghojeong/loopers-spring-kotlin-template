# 10주 여정: "CRUD 개발자"에서 "시스템을 설계하는 엔지니어"로

## TL;DR

10주 동안 진짜 많이 배웠다. **설계 → 동시성 → 성능 → 회복력 → 이벤트 → 확장성 → 데이터 파이프라인**까지, Quest로 배운 걸 Bithumb 실무에 바로 적용하면서 "일단 돌아가게만 만드는 개발자"에서 "왜 이렇게 만들었는지 설명할 수 있는 엔지니어"가 되어가고 있다. 가장 크게 느낀 건, **"은탄환은 없고, Trade-off를 이해하고 맥락에 맞게 선택하는 게 진짜 실력"**이라는 것.

---

## 전체 여정 요약

### 1~3주차: 기초 다지기 - "코드가 아니라 구조를 먼저 생각하라"

솔직히 처음엔 자신감이 없었다. "TDD? 들어는 봤는데... 테스트 쓰는 게 귀찮지 않나?" 싶었다.

근데 첫 주에 테스트를 작성하면서 완전히 생각이 바뀌었다. 테스트하기 어려운 코드는 설계가 잘못된 거였고, 테스트가 쉬운 코드는 구조가 명확한 거였다. **테스트가 내 설계의 거울**이었던 거다. TestContainers로 실제 DB랑 비슷한 환경에서 테스트하고, Mock/Stub/Fake를 언제 써야 하는지 고민하면서 많이 배웠다.

2주차에는 더 큰 충격을 받았다. Bithumb에서 KYC(고객확인) 시스템 문서를 읽는데, 도대체 무슨 말인지 하나도 모르겠는 거다. KYC, AML, EDD, CDD... 금융권 용어를 이해하고 나서야 "아, 이걸 만들어야 하는구나"가 보이기 시작했다.

**"도메인을 모르면 코드를 아무리 잘 써도 소용없구나."**

3주차 도메인 모델링에서는 그 깨달음을 코드로 표현하는 법을 배웠다. Entity와 VO를 나누고, Domain Service로 비즈니스 로직을 표현하니까 **"코드가 비즈니스를 말하는"** 느낌이 들었다.

이 3주를 통해 깨달았다. 코드 짜기 전에 구조를 먼저 고민하고, 도메인을 이해하는 게 진짜 개발의 시작이라는 것을.

---

### 4~6주차: 현실과 마주하기 - "이론과 실전의 간극"

4주차에서 처음으로 성능이라는 벽에 부딪혔다. Bithumb 입사 첫 평가였는데, **600만 건의 회원 데이터를 배치 처리**해야 했다. "뭐 그냥 `@Transactional` 붙이고 `findAll()` 하면 되지 않나?" 싶었는데... DB CPU가 95%까지 치솟으면서 서비스가 느려지기 시작했다.

"이건 큰일났다..."

멘토님이 힌트를 주셨다. "조회만 하는 거면 `READ_ONLY` 트랜잭션을 써봐." 단순한 설정 하나였는데, 효과는 놀라웠다. DB CPU가 15%로 뚝 떨어지고, HikariCP 풀도 안정화됐다.

**첫 번째 Trade-off 깨달음:** READ_WRITE는 락을 잡아서 서비스에 영향을 주지만, READ_ONLY는 락이 없어서 안전하다. 조회만 하는 배치에서는 READ_ONLY가 답이었다.

5주차는 더 극적이었다. SBS 가요대전 방청권 티케팅 시스템을 만들었는데, **초당 17,000건의 신청**이 몰려왔다. DB만으로는 도저히 감당이 안 됐다. Redis를 도입하고, `SETNX`로 중복 신청을 막고, Write-Behind 패턴으로 Redis에 먼저 저장하고 DB에는 나중에 쓰는 방식으로 버텼다. 손이 떨렸지만, 배운 대로 했더니 됐다.

**두 번째 Trade-off 깨달음:** Redis는 빠르지만 휘발성이니까, Write-Behind로 결국 DB에 저장해서 성능과 안정성을 둘 다 잡았다.

그리고 6주차... **7분간의 프로덕션 장애**. 진짜 잊을 수 없는 경험이었다.

결제 PG사 API가 느려지자 우리 서비스 전체가 멈췄다. 뉴스에도 나고, 고객 문의가 폭주했다. 그때 멘토님이 말씀하셨다. "Circuit Breaker가 있었으면 이런 일은 없었을 거야."

그날 밤에 Resilience4j를 공부했다. Circuit Breaker, Timeout, Fallback... 외부 시스템은 언제든 실패한다는 걸 뼈저리게 느꼈다.

**세 번째 Trade-off 깨달음:** Circuit Breaker를 달면 복잡해지긴 하지만, 장애 전파를 막으려면 필수다. 특히 결제, 인증 같은 중요한 외부 시스템은 반드시 보호해야 한다.

이 3주를 통해 깨달았다. 책에서 배운 이론은 현실의 트래픽과 장애 앞에서 완전히 다르게 느껴진다는 것을. 성능과 안정성은 선택이 아니라 필수라는 것을.

---

### 7~9주차: 사고방식의 전환 - "이벤트로 생각하기"

7주차부터 세계관이 바뀌었다. `ApplicationEvent`를 배우면서 "이벤트 기반으로 생각하면 코드가 진짜 심플해지는구나"를 경험했다. 티케팅 시스템에서 신청 완료 후 이메일 발송, 포인트 적립 등을 동기로 처리하니까 응답 시간이 2.5초나 걸렸다. `@Async`로 분리하자 **80ms로 줄어들었다. 30배나 빨라진 거다.**

"이거면 충분하지 않나?" 싶었는데, 곧 한계를 발견했다.

데이터 플랫폼팀에 이벤트를 전달해야 하는데, `ApplicationEvent`는 같은 JVM 안에서만 동작한다는 거다. 서버가 죽으면 이벤트도 날아간다. 금융 서비스에서 신청 내역이 사라지면 큰일이다.

8주차에 Kafka를 배우면서, 세계가 또 한 번 바뀌었다.

**"Kafka는 메시지 큐가 아니라 분산 로그 저장소구나."**

메시지가 디스크에 영구 저장되고, Offset으로 언제든 추적할 수 있었다. Transactional Outbox 패턴을 적용하니까 **"DB에 기록된 건 반드시 Kafka에도 전송"**을 보장할 수 있었다.

실제로 고객이 "신청했는데 내역이 없어요"라고 문의했을 때, Kafka Offset을 전수 조사해서 이벤트는 발행됐지만 Consumer가 실패했다는 걸 밝혀냈다. 일반 로그였다면 불가능했을 추적이었다.

**네 번째 Trade-off 깨달음:** ApplicationEvent는 심플하지만 분산 환경에서 한계가 있다. Kafka는 복잡하지만 이벤트 소싱, 감사 추적, 장애 복구의 마지막 보루다. 금융 서비스처럼 데이터 유실이 허용되지 않는 곳에서는 Kafka가 필수다.

9주차는 또 다른 충격이었다. SBS 가요대전 실시간 투표 랭킹 시스템을 만들면서 깨달았다.

**"랭킹은 데이터 구조 문제가 아니라 시간 관리 문제구나."**

처음엔 "Redis ZSET 쓰면 끝 아니야?" 싶었는데, 3일 후 확인하니 첫날 1위가 계속 1위였다. 초반에 많이 받은 아이돌이 계속 유리한 **Long Tail Problem**이었다.

해결책은 **Time Quantization**이었다. 전체/일간/시간별로 키를 분리하니까 "지금 핫한 아이돌"이 보였다. 근데 새 날이 시작되면 순위가 텅 비는 **Cold Start Problem**이 생겼다.

**다섯 번째 Trade-off 깨달음:** 어제 순위의 10%를 오늘 순위에 미리 반영했다. 0%면 빈 순위가 노출되고, 100%면 Long Tail이 재발한다. 10%가 사용자 경험과 공정성의 균형점이었다.

그리고 투표 시작 3일 후, 새벽 2시에 갑자기 1만 표가 들어왔다. "뭔가 이상한데?" Kafka로 전수 조사하니까 매크로 프로그램을 통한 부정 투표였다. Kafka가 없었다면 증거를 찾을 수 없었을 거다.

이 3주를 통해 깨달았다. 이벤트 기반 사고는 단순히 코드를 분리하는 게 아니라, 시스템의 변화를 추적하고, 장애를 복구하며, 부정을 탐지하는 기반이라는 것을.

---

### 10주차: 데이터 엔지니어링 - "대량 데이터는 다르다"

마지막 주는 Spring Batch를 배우면서 "대량 데이터 처리는 완전히 다른 세계구나"를 깨달았다.

Bithumb에서 금감원 규제 대응을 위해 **600만 회원 + 1,500만 고객**의 정합성을 체크하는 배치를 만들어야 했다. "뭐 `@Scheduled`로 `findAll()` 하면 되지 않나?" 싶었는데...

결과는 참담했다. **30분 걸리고, 메모리 12GB 먹고, DB CPU 95%까지 치솟고, 서비스가 느려졌다.**

"이건 안 되겠다..."

Quest에서 배운 **Chunk-Oriented Processing**을 적용했다. 100개씩 나눠서 읽고(ItemReader), 처리하고(ItemProcessor), 쓰는(ItemWriter) 방식으로 바꿨다.

근데 RepositoryItemReader의 페이징은 여전히 느렸다. `OFFSET 6,000,000`을 처리하려면 DB가 600만 건을 건너뛰어야 하니까. **JdbcCursorItemReader**로 바꾸니까 포인터만 이동해서 속도가 일정했다.

**여섯 번째 Trade-off 깨달음:** 페이징은 구현이 쉽지만 대량 데이터에서 느리다. 커서는 JDBC 타입 설정이 필요하지만 성능이 일정하다. 100만 건 이상은 커서가 답이다.

그리고 4주차에 배운 READ_ONLY를 다시 적용했다. 배치가 30분 동안 락을 잡고 있어서 서비스가 느려졌는데, READ_ONLY로 바꾸니까 락이 사라지고 DB CPU가 15%로 떨어졌다.

최종 결과: **30분 → 3분 (10배 개선), 메모리 12GB → 5GB, DB CPU 95% → 15%, 서비스 영향 없음.**

신기한 건, 4주차에 배운 READ_ONLY를 10주차에 다시 쓰면서 "학습은 반복되고 심화되는구나"를 느꼈다는 거다. 같은 개념도 맥락에 따라 다르게 적용된다.

---

## 가장 큰 전환점

### 1. Week 3: "도메인 지식은 코딩 실력만큼 중요하다"

처음엔 "나는 개발자니까 코드만 잘 쓰면 되지 않나?" 싶었다. 근데 Bithumb에서 KYC 시스템을 만들면서 생각이 완전히 바뀌었다. KYC, AML, EDD, CDD... 금융 용어를 하나도 모르니까, 아무리 코드를 잘 써도 **"무엇을 만들어야 하는지"**를 모르겠는 거다.

Entity와 VO를 나누고, Domain Service로 비즈니스 로직을 표현하는 것도 결국 **"도메인을 코드로 말하는 것"**이었다. 그 이후로는 항상 문서를 먼저 읽고, 도메인 전문가랑 대화하며 Ubiquitous Language를 찾으려고 노력했다.

### 2. Week 6: "장애는 반드시 온다. 준비하지 않으면 재앙이다"

7분간의 프로덕션 장애는 내 커리어에서 가장 충격적인 경험이었다. 결제 PG사 API가 느려지자 우리 서비스 전체가 멈췄다. 뉴스에도 나고, 고객이 분노했다.

그 전까지는 "잘 만들면 장애가 안 날 거야"라고 생각했다. 근데 **외부 시스템은 언제든 실패한다.** Circuit Breaker, Timeout, Fallback... 이런 게 선택이 아니라 필수라는 걸 그날 뼈저리게 느꼈다. 이후로는 모든 외부 호출을 의심하고, 장애 전파를 막는 걸 최우선으로 설계하게 됐다.

### 3. Week 7→8: "ApplicationEvent의 한계를 넘어 Kafka로"

ApplicationEvent로 이벤트를 분리하면서 "이거면 충분하다"고 생각했다. 근데 데이터 플랫폼팀에 이벤트를 보내야 하는 순간, "같은 JVM 안에서만 동작한다"는 한계를 발견했다.

Kafka를 배우면서 **"분산 시스템에서는 이벤트가 영구 저장되고, 추적 가능해야 한다"**는 것을 알게 됐다. Transactional Outbox, Idempotent Consumer, DLQ 같은 패턴들이 왜 필요한지 체감했다.

이 전환점이 내 사고방식을 **"하나의 서버"에서 "분산 시스템"**으로 확장시켰다.

### 4. Week 9: "랭킹은 데이터 구조가 아니라 시간 관리 문제"

"Redis ZSET 쓰면 끝 아니야?"라고 생각했는데, Long Tail Problem을 마주하면서 완전히 다른 세계라는 걸 깨달았다.

누적 투표는 초반 인기가 계속 유리하다. 해결책은 **시간을 어떻게 나누고, 얼마나 반영하느냐**였다. 전체/일간/시간별로 키를 분리하고, 10% Score Carry-Over로 Cold Start를 방지하는 설계를 하면서 "랭킹의 목적은 숫자가 아니라 **팬들이 응원하는 재미**"라는 걸 이해하게 됐다.

---

## 나의 Trade-off 판단

모든 기술 선택은 Trade-off다. 정답은 없고, **맥락에 맞는 선택**만 있다.

### 1. READ_ONLY 트랜잭션 (Week 4, 10)
- **선택지:** READ_WRITE vs READ_ONLY
- **내 판단:** 조회 중심 배치에서는 READ_ONLY로 락을 제거했다.
- **왜 그랬나:** 데이터 변경이 없으니까 락이 불필요했다. DB CPU가 95%에서 15%로 뚝 떨어졌다.
- **다시 한다면:** 동일하게 선택하겠다. 실서비스에 영향을 주지 않는 게 최우선이니까.

### 2. Redis 중복 방지 (Week 5)
- **선택지:** DB UNIQUE 제약 vs Redis SETNX
- **내 판단:** Redis SETNX로 초당 17,000건을 처리했다.
- **왜 그랬나:** DB는 중복 체크 시 락이 발생하는데, Redis는 메모리 기반이라 빠르고 락이 없다.
- **Trade-off:** Redis는 휘발성이니까 Write-Behind로 결국 DB에 저장해서 성능과 안정성을 둘 다 잡았다.
- **다시 한다면:** 동일하게 선택하겠다. 단, Redis 장애 대비 Fallback을 추가하고 싶다.

### 3. Kafka vs ApplicationEvent (Week 8)
- **선택지:** ApplicationEvent vs Kafka
- **내 판단:** 금융 서비스에서는 Kafka를 선택했다.
- **왜 그랬나:**
  - ApplicationEvent: 심플하지만 JVM 재시작하면 이벤트 날아감
  - Kafka: 복잡하지만 이벤트 영구 저장되고, Offset 추적되고, 전수 조사 가능
- **Trade-off:** Kafka는 인프라 비용과 학습 곡선이 높지만, 신청 누락이 허용되지 않는 곳에서는 필수다.
- **다시 한다면:** 동일하게 선택하겠다. 실제로 Kafka Offset 추적으로 누락 건을 복구했으니까.

### 4. 10% Score Carry-Over (Week 9)
- **선택지:** 0% (완전히 새로운 순위) vs 10% vs 100% (어제 그대로)
- **내 판단:** 10%로 설정했다.
- **왜 그랬나:**
  - 0%: 빈 순위 노출돼서 사용자 경험 나쁨
  - 10%: 어제 인기의 흔적 + 오늘 트렌드 반영
  - 100%: Long Tail Problem 재발
- **Trade-off:** 사용자 경험(빈 순위 방지)과 공정성(새로운 인기 반영)의 균형점.
- **다시 한다면:** 5~15% 범위에서 A/B 테스트를 해보고 싶다.

### 5. Cursor vs Paging (Week 10)
- **선택지:** RepositoryItemReader(페이징) vs JdbcCursorItemReader(커서)
- **내 판단:** 커서를 선택했다.
- **왜 그랬나:**
  - 페이징: `OFFSET 6,000,000`은 DB가 600만 건을 건너뛰니까 엄청 느림
  - 커서: 포인터만 이동하니까 속도가 일정함
- **Trade-off:** 커서는 JDBC 설정(TYPE_FORWARD_ONLY)이 필요하지만, 성능이 훨씬 낫다.
- **다시 한다면:** 동일하게 선택하겠다. 100만 건 이상은 커서가 답이다.

**공통된 교훈:** "왜 이 선택을 했는가"를 설명할 수 있어야 한다. 근거 없는 선택은 그냥 운에 맡기는 거다.

---

## 실전과의 연결

Quest로 배우고 즉시 Bithumb 실무에 적용하는 과정이 이 10주의 가장 큰 가치였다.

### Week 4 Quest → Bithumb 입사 평가
- **Quest:** 트랜잭션 격리 수준과 READ_ONLY 최적화
- **실무:** 600만 회원 배치 처리, READ_ONLY로 DB CPU 95% → 15%
- **연결:** 이론을 배우고 즉시 적용하니까 "왜 필요한지"가 체감됐다.

### Week 5 Quest → SBS 가요대전 티케팅
- **Quest:** Redis 캐싱, Write-Behind 패턴
- **실무:** 초당 17,000건 신청, Redis SETNX로 중복 방지
- **연결:** 책에서만 보던 "대용량 트래픽"을 직접 경험했다. 손이 떨렸지만, 배운 대로 했더니 됐다.

### Week 6 학습 → 프로덕션 장애
- **학습:** Circuit Breaker, Fallback, Resilience4j
- **실무:** 7분 장애 후 Circuit Breaker 도입, 재발 방지
- **연결:** "장애는 교육이다." 아프지만 가장 많이 배웠다.

### Week 8 Quest → SBS 티케팅 Kafka 적용
- **Quest:** Kafka, Transactional Outbox, Idempotent Consumer
- **실무:** 신청 누락 방지, Offset 추적으로 전수 조사
- **연결:** "신청했는데 내역이 없어요"라는 고객 문의를 Kafka로 해결했다. Kafka가 없었다면 불가능했을 추적이었다.

### Week 9 Quest → SBS 투표 시스템
- **Quest:** 랭킹 시스템, Time Quantization, Cold Start 방지
- **실무:** 127만 표 처리, 부정 투표 1만 표 적발
- **연결:** Kafka Offset으로 매크로 투표 패턴을 발견했다. 이벤트 소싱이 감사 추적의 기반이라는 걸 깨달았다.

### Week 10 Quest → Bithumb 정합성 배치
- **Quest:** Spring Batch, Chunk-Oriented Processing, Delete-then-Insert
- **실무:** 30분 → 3분 (10배 개선), 서비스 영향 없음
- **연결:** Quest에서 배운 패턴을 실무에 적용하니까 금감원 규제 리스크를 관리할 수 있게 됐다.

**핵심:** "배운 걸 쓸모없다고 생각하지 말고, 실무에 어떻게 적용할지 고민하는 게 진짜 학습이다."

---

## 10주 후 달라진 것

### Before (10주 전)
- TDD? "귀찮은 일"
- 트랜잭션? "@Transactional 붙이면 되는 거 아냐?"
- 성능? "일단 작동하게 만들고 보자"
- 장애? "잘 만들면 안 날 거야"
- 이벤트? "그게 뭔데?"
- Kafka? "메시지 큐 아냐?"
- 배치? "for 문 돌리면 되는 거 아냐?"

### After (10주 후)
- TDD: "테스트는 내 설계의 거울이다"
- 트랜잭션: "READ_ONLY로 락을 제거해야 서비스 영향이 없다"
- 성능: "Redis, HikariCP, 커서, 청크... 맥락에 맞는 도구를 선택하자"
- 장애: "외부 시스템은 반드시 실패한다. Circuit Breaker는 필수다"
- 이벤트: "시스템의 변화를 추적하고, 장애를 복구하는 기반이다"
- Kafka: "분산 로그 저장소이자, 감사 추적의 마지막 보루다"
- 배치: "대량 데이터는 Chunk-Oriented Processing으로 나눠야 한다"

**가장 큰 변화:** "일단 돌아가게"가 아니라, **"왜 이 선택을 했는가"**를 설명할 수 있게 됐다.

---

## 앞으로의 여정

10주는 끝났지만, 성장은 여기서부터 시작이다.

아직도 모르는 게 너무 많다. 분산 트랜잭션(Saga), 캐시 일관성, 대규모 데이터 샤딩, Kubernetes 기반 배포... 배울 게 산더미다.

하지만 이제는 안다. **"어떻게 배워야 하는지"**를.

1. **문제를 먼저 만나라.** 이론만 공부하면 금방 잊는다. 실전 문제를 만나고, 해결하며 배워야 기억에 남는다.

2. **Trade-off를 이해하라.** 은탄환은 없다. 각 기술의 장단점을 알고, 맥락에 맞게 선택하는 게 진짜 실력이다.

3. **작은 것부터 적용하라.** Spring Batch를 배웠다고 모든 배치를 다시 짤 필요는 없다. READ_ONLY 하나만 적용해도 큰 효과가 있다.

4. **실패를 두려워하지 마라.** 7분 장애는 아팠지만, 가장 많이 배웠다. 실패는 교육이다.

5. **왜 그런지 고민하라.** "왜 커서가 페이징보다 빠른가?", "왜 Kafka는 메시지를 디스크에 저장하는가?" 원리를 이해해야 응용할 수 있다.

---

## 마치며

10주 전, 나는 "CRUD 개발자"였다. 기능을 만들고, 버그를 고치고, PR을 올렸다.

10주 후, 나는 "시스템을 설계하는 엔지니어"가 되고 있다. Trade-off를 판단하고, 장애를 예방하며, 확장 가능한 구조를 고민한다.

이 변화는 단순히 "기술을 많이 배워서"가 아니다. **"왜 그렇게 판단했는가"를 고민하는 습관** 덕분이다.

앞으로도 계속 고민하고, 적용하고, 실패하고, 배우겠다.

**"성장은 목적지가 아니라 여정이다."**

그리고 이 10주는 그 여정의 시작이었다.
